1.A: Get the MNIST digit data set

1.B: Build a network model
* TODO: plot the architecture (1-B)

1.C: Train the model
Based on the plot, training accuracy has exponentially increased as well as the test accuracy. It seems like there is no overfitting.
We could say it as an example of benign overfitting as model we built generalizes well to the test set.

1.E: Read the network and run it on the test set
Our model has learned the pattern of the handwritten numbers quite well. For the 9th image, it seems like it can be both 
interpreted as 5 or 6, and our model has classified it as 5 instead of 6.

1.F: Test the network on new inputs
Our model has successfully classified all the digits except for the 0 digit. It classified it as 9 instead of 0. It seems likely
that if the digit was written thicker the result would have been more accurate. Also, I believe the model weight that we trained on
had less stronger edge detection on the lower part where the digit 9 and 0 makes difference.

2.A: Analyze the first layer
Each filter is an output of the first convolution layer of size 5x5 where each patch in the original image had 10 different
channel filters, meaning each channel filter had different functionality of gathering the feature of that specific patch. It seems like for each filter, some of the areas have higher contrast then rest of the area(i.e: filter1 (3,3)).
It is similar to the sobelX, sobelY-like gradient filters for edge detection. 


3. Transfer Learning on Greek Letters
After running 15 epochs, we have learned that something around 5~6 epochs were enough to do the transfer learning.

For running the experiment on our training data, it has successfully classfied all three variation as can be seen in the image.

4. MNIST Network Architecture Experiment
This experiment aims to analyze how changes to the neural network architecture affect the performance of MNIST digit recognition. We'll systematically vary three key dimensions of the network architecture to understand their impact on both accuracy and training. We analyze number of convolutional filters, dropout rate, and batch size to see the impact on performance. Hypothesis for each of these is as follows:

Number of Convolutional Filters

Hypothesis: Increasing the number of convolutional filters will improve model accuracy up to a point, after which we'll see diminishing returns. However, more filters will always increase training time approximately linearly.

Dropout Rate

Hypothesis: A moderate dropout rate (around 0.25) will yield the best test accuracy. Too low (0.1) and the model might overfit; too high (0.5) and it might struggle to learn effectively. Training time should be relatively unaffected by dropout rate.

Batch Size

Hypothesis: Larger batch sizes will train faster in wall-clock time but may achieve slightly lower accuracy. The optimal batch size for accuracy will likely be in the middle range (64-128).