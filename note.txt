1.A: Get the MNIST digit data set

1.B: Build a network model
* TODO: plot the architecture (1-B)

1.C: Train the model
Based on the plot, training accuracy has exponentially increased as well as the test accuracy. It seems like there is no overfitting.
We could say it as an example of benign overfitting as model we built generalizes well to the test set.

1.E: Read the network and run it on the test set
Our model has learned the pattern of the handwritten numbers quite well. For the 9th image, it seems like it can be both 
interpreted as 5 or 6, and our model has classified it as 5 instead of 6.

2.A: Analyze the first layer
Each filter is an output of the first convolution layer of size 5x5 where each patch in the original image had 10 different
channel filters, meaning each channel filter had different functionality of gathering the feature of that specific patch. It seems like for each filter, some of the areas have higher contrast then rest of the area(i.e: filter1 (3,3)).
It is similar to the sobelX, sobelY-like gradient filters for edge detection. 


3. Transfer Learning on Greek Letters
After running 15 epochs, we have learned that something around 5~6 epochs were enough to do the transfer learning. 